# AIdea Lab: Claude를 위한 상세 구현 계획

## I. 일반 지침 (개발 착수 전 필독)

1.  **문서 숙지 (.cursor/rules/rule.mdc I.1, I.2)**:
   
    * 코딩 시작 전, 항상 `memory-bank/@architecture.md` (AIdea Lab 아키텍처 상세 원본) 파일과 `memory-bank/@workflow.md` (본 구현 계획의 기반이 된 "AIdea Lab: 아키텍처 기반 상세 구현 워크플로우 (번호 수정 및 재검토)" 파일)를 반드시 정독하고 완전히 이해해야 합니다.
    * 본 구현 계획은 위 두 문서를 기반으로 작성되었으므로, 맥락 이해를 위해 원본 문서들을 함께 참조하십시오.
2.  **.cursor/rules/rule.mdc 준수**:
    * 본 계획의 모든 개발 과정에서 제공된 "Project Development Rules &Guidelines" (커서룰) 전체를 반드시 준수해야 합니다. 특히 다음 사항들을 유념하십시오:
        * **모듈식 설계 및 파일 구조 (.cursor/rules/rule.mdc II)**: 모든 코드는 기능 단위로 분리된 여러 파일로 모듈화합니다. 단일 파일은 500줄을 넘지 않도록 고려하고, 단일 책임 원칙을 지킵니다. 모놀리식 구조는 엄격히 금지합니다. 관련된 기능은 동일 디렉토리에 그룹화하고, 각 모듈은 명확한 책임과 경계를 가집니다.
        * **아키텍처 문서 업데이트 (.cursor/rules/rule.mdc I.3)**: 주요 기능 추가 또는 마일스톤 완료 후에는 반드시 `memory-bank/@architecture.md` 파일을 최신 상태로 업데이트합니다.
        * **코드 작성 및 디버깅 (.cursor/rules/rule.mdc IV, VI)**: 일관된 코딩 스타일, 적절한 주석, 복잡한 로직 분해, 디버거 및 로깅의 적극적 활용, 변경 후 항시 테스트를 생활화합니다.
3.  **점진적 구현 및 테스트**: 각 단계별 지침을 따르고, 각 단계 완료 후 명시된 검증 테스트를 반드시 수행하여 문제 발생 시 즉시 해결합니다. (커서룰 VI.1, VI.3)
4.  **AI 활용 (.cursor/rules/rule.mdc V)**: AI에게 코드 생성을 요청할 경우, 명확하고 구체적인 요구사항을 제시하고, 생성된 코드는 항상 리뷰 후 적용합니다. 복잡한 문제는 단계별로 설명하여 도움을 받습니다.

---

## Phase 0: 준비 및 환경 설정 (필수 선행)

* **목표**: AIdea Lab 개발을 위한 완벽한 환경을 구축하고, Google ADK와 선택된 Gemini LLM API의 기본 연동 및 작동을 확실하게 검증합니다.
* **참고 워크플로우 항목**: Phase 0 전체

### 단계 0.1: Python 가상 환경 설정 및 프로젝트 구조 초기화
    * **지침**:
        1.  `src` (소스 코드), `tests` (테스트 코드), `docs` (문서), `config` (설정 파일), `scripts` (유틸리티 스크립트) 등의 기본 디렉토리 구조 생성. (커서룰 II.5, III.1, III.2)
    * **검증**:
        * 생성된 디렉토리 구조가 올바른지 확인.

### 단계 0.2: 필수 라이브러리 설치 및 관리
    * **지침**:
        1.  프로젝트 루트에 `requirements.txt` 파일 생성.
        2.  `requirements.txt`에 다음 라이브러리들을 명시 (버전은 아래버전):
            * `google-generativeai>=0.3.1`
            * `google-adk>=0.4.0`
            * `streamlit>=1.31.0`
            * `python-dotenv>=1.0.0`
            * `pytest>=7.4.3`
            * `google-adk`
            * `google-generativeai` (Gemini API 사용 시) 또는 관련 Google Cloud AI 라이브러리
            * `streamlit` 또는 `gradio` (UI 도구 최종 선택 후 하나만)
            * `python-dotenv` (API 키 등 환경변수 관리용)
            * 기타 필요한 라이브러리 (필요시 추가)
        3.  `pip install -r requirements.txt` 명령으로 라이브러리 설치.
    * **참고 아키텍처**: 워크플로우 Phase 0 - 항목 1 (기술 스택)
    * **검증**:
        * 모든 라이브러리가 오류 없이 설치되는지 확인.
        * `pip freeze` 명령으로 설치된 라이브러리 및 버전 확인.

### 단계 0.3: Google Cloud SDK 설치 및 인증 (필요시)
    * **지침**:
        1.  Gemini API 접근 또는 향후 Google Cloud 서비스(예: Vertex AI Agent Engine) 사용을 위해 Google Cloud SDK 설치가 필요한 경우, 공식 문서에 따라 설치.
        2.  `gcloud init` 및 `gcloud auth application-default login` 명령을 실행하여 개발 환경 인증.
    * **참고 아키텍처**: 워크플로우 Phase 0 - 항목 3 (Google ADK 설치...)
    * **검증**:
        * `gcloud --version` 명령으로 SDK 설치 및 버전 확인.
        * 인증 명령 실행 후 성공 메시지 확인.

### 단계 0.4: API 키 및 환경 설정
    * **지침**:
        1.  프로젝트 루트에 `.env` 파일 생성.
        2.  `.env` 파일에 Gemini API 사용을 위한 `GOOGLE_API_KEY="YOUR_API_KEY"` (또는 서비스 계정 JSON 경로 등) 설정. (실제 키 값은 포함하지 말 것)
        3.  `.gitignore` 파일에 `.env` 파일을 추가하여 Git 저장소에 포함되지 않도록 설정.
    * **참고 아키텍처**: 워크플로우 Phase 0 - 항목 2 (LLM 모델...), 항목 3 (Google ADK 설치...)
    * **검증**:
        * Python 스크립트에서 `python-dotenv`를 사용하여 `.env` 파일의 API 키를 정상적으로 로드할 수 있는지 테스트 (실제 API 호출은 다음 단계).

### 단계 0.5: Google ADK 기본 LlmAgent 예제 실행 (LLM 연동 검증)
    * **지침**:
        1.  Google ADK 공식 문서에서 제공하는 가장 간단한 `LlmAgent` (또는 유사 기본 에이전트) 예제 코드를 가져와 `src/poc` (Proof of Concept) 디렉토리에 테스트용 파일로 생성 (예: `src/poc/simple_adk_agent.py`).
        2.  예제 코드 내 LLM 모델 설정을 Phase 0 - 항목 2에서 확정한 Gemini 모델로 변경.
        3.  API 키가 환경 변수에서 올바르게 로드되어 사용되는지 확인.
        4.  간단한 프롬프트를 사용하여 `LlmAgent`를 실행하고, LLM으로부터 응답을 받아 출력하는지 확인.
    * **참고 아키텍처**: 워크플로우 Phase 0 - 항목 3 (...기본 작동 검증)
    * **커서룰**: II.4 (모놀리식 구조 지양 - 예제 코드라도 너무 길면 분리 고려)
    * **검증**:
        * 스크립트 실행 시 LLM API 호출 성공 및 의미 있는 응답 수신 확인.
        * API 키 오류, 인증 오류 등이 발생하지 않는지 확인.

### 단계 0.6: ADK `session.state` 기본 사용법 테스트
    * **지침**:
        1.  `src/poc` 디렉토리에 `session_state_test_agent.py` 파일 생성.
        2.  간단한 `LlmAgent`를 정의하고, `session.state`에 특정 값을 저장하는 로직 추가 (예: `session.state['test_value'] = "hello"`).
        3.  `LlmAgent`의 `instruction` (또는 프롬프트 템플릿) 내에서 `{state.test_value}`와 같이 `session.state` 값을 참조하여 LLM 프롬프트를 구성.
        4.  에이전트 실행 후, LLM 응답에 `session.state` 값이 올바르게 반영되었는지 확인.
    * **참고 아키텍처**: 워크플로우 Phase 0 - 항목 3 (...기본 작동 검증)
    * **검증**:
        * LLM 응답에 "hello" (또는 설정한 값)가 포함되어 출력되는지 확인.
        * `session.state` 값 참조 시 오류가 발생하지 않는지 확인.

---

## Phase 1: 단일 페르소나 아이디어 분석 (핵심 기능의 최소 단위 구현)

* **목표**: 사용자가 아이디어를 입력하면, 미리 정의된 단일 AI 페르소나가 해당 아이디어를 분석하고, 그 결과를 UI를 통해 사용자에게 보여주는 가장 기본적인 End-to-End 흐름을 완성합니다.
* **참고 워크플로우 항목**: Phase 1 전체



### 단계 1.2: 단일 페르소나 ADK `LlmAgent` 클래스 생성
    * **지침**:
        2.  선택된 페르소나를 위한 Python 파일 생성 (예: `src/agents/critic_agent.py`). (커서룰 III.2)
        3.  해당 파일에 Google ADK의 `LlmAgent` (또는 해당 기본 에이전트 클래스)를 상속받는 페르소나 에이전트 클래스 정의 (예: `CriticPersonaAgent`). (커서룰 II.1, II.3)
    * **검증**:
        * 클래스가 오류 없이 임포트되고 기본 인스턴스화가 가능한지 확인 (기능은 다음 단계에서 구현).

### 단계 1.3: `LlmAgent`에 시스템 프롬프트 및 LLM 파라미터 설정
    * **지침**:
        1.  단계 1.2에서 생성한 페르소나 에이전트 클래스(`CriticPersonaAgent`)의 생성자(`__init__`) 또는 별도 설정 메서드 내에서 다음을 설정:
            * `model`: Phase 0에서 확정한 Gemini 모델명 사용.
            * `instruction`: 단계 1.1에서 `config/prompts.py`에 저장한 시스템 프롬프트를 로드하여 설정. 이 `instruction`은 사용자 아이디어를 받을 수 있도록 템플릿 변수(예: `{state.user_idea}`)를 포함해야 함.
            * LLM 파라미터: 워크플로우 Phase 1 - 항목 4 (`API 호출 시 파라미터 최종 값 결정`)를 참고하여 `temperature`, `max_output_tokens` 등 초기값 설정.
    * **커서룰**: II.3 (단일 책임 원칙 - 생성자는 초기화, 설정 메서드는 설정)
    * **검증**:
        * 에이전트 인스턴스 생성 시, 시스템 프롬프트와 LLM 파라미터가 정상적으로 내부 변수에 할당되는지 디버거로 확인.

### 단계 1.4: `LlmAgent`의 입력(`session.state` 참조) 및 출력(`session.state` 저장) 로직 구현
    * **지침**:
        1.  `CriticPersonaAgent` 클래스에서 ADK의 실행 로직(예: `run` 또는 `process` 메서드 오버라이드)을 활용하여 다음을 구현:
            * `session.state`에서 사용자의 아이디어를 저장할 키(예: `initial_idea`)를 정의하고, 에이전트 실행 시 해당 키를 통해 아이디어를 가져와 `instruction` 템플릿에 주입.
            * LLM으로부터 받은 응답을 `session.state`에 저장할 `output_key`(예: `critic_response`)를 정의하고, 해당 키로 응답 텍스트를 `session.state`에 저장.
    * **참고 아키텍처**: 워크플로우 Phase 1 - 항목 5 (`ADK 내부 에이전트 간 데이터 구조...`, `상태 관리...`)
    * **검증**:
        * 단위 테스트 작성: 가상의 `session.state` 객체와 `initial_idea`를 주입했을 때, 에이전트가 올바른 프롬프트를 생성하고, LLM (Mock 사용 가능) 호출 후 응답을 지정된 `output_key`로 `session.state`에 저장하는지 검증. (실제 LLM 호출 테스트는 통합 테스트에서)

### 단계 1.5: Streamlit/Gradio 기본 UI 파일 및 구조 생성
    * **지침**:
        1.  `src/ui` 디렉토리 생성. (커서룰 II.5)
        2.  선택한 UI 도구(Streamlit 또는 Gradio)를 사용하여 기본 애플리케이션 파일 생성 (예: `src/ui/app.py`).
        3.  UI 코드와 백엔드 로직(에이전트 호출 등)을 분리하기 위한 기본 구조 구상. (커서룰 II.1)
    * **검증**:
        * `streamlit run src/ui/app.py` (또는 Gradio 실행 명령) 실행 시, 빈 UI 화면이라도 오류 없이 정상적으로 실행되는지 확인.

### 단계 1.6: UI에 아이디어 입력 필드 및 분석 요청 버튼 구현
    * **지침**:
        1.  `src/ui/app.py`에 다음 UI 요소 구현 (워크플로우 Phase 1 - 항목 6 (`주요 화면 구성 및 기능`) 참고):
            * 사용자가 아이디어를 입력할 수 있는 텍스트 입력 영역 (`st.text_area` 또는 `gr.Textbox`).
            * "아이디어 분석 요청" 버튼 (`st.button` 또는 `gr.Button`).
    * **검증**:
        * UI 실행 후, 텍스트 입력 영역과 버튼이 화면에 정상적으로 표시되는지 확인.
        * 텍스트 입력 및 버튼 클릭이 가능한지 확인.

### 단계 1.7: UI에서 ADK 에이전트 실행 및 결과 표시 로직 구현
    * **지침**:
        1.  "아이디어 분석 요청" 버튼 클릭 시 호출될 함수를 `src/ui/app.py`에 구현.
        2.  해당 함수 내에서 다음 로직 수행:
            * UI 텍스트 입력 영역의 값을 가져옴.
            * ADK 세션(`session` 또는 `state` 객체)을 초기화하거나 가져옴.
            * 입력된 아이디어를 `session.state['initial_idea']` (또는 단계 1.4에서 정의한 키)에 저장.
            * 단계 1.2에서 생성한 `CriticPersonaAgent` 인스턴스를 생성하고 실행 (ADK의 에이전트 실행 방식 따름).
            * 에이전트 실행 후, `session.state['critic_response']` (또는 단계 1.4에서 정의한 `output_key`)에서 결과 텍스트를 가져옴.
            * 결과 텍스트를 UI에 표시 (`st.write`, `st.markdown` 또는 `gr.Textbox` 등).
    * **참고 아키텍처**: 워크플로우 Phase 1 - 항목 7 (`시스템 아키텍처 (단순화된 초기 형태)`)
    * **커서룰**: II.1, II.3 (UI 로직과 에이전트 호출 로직 분리 고려)
    * **검증**:
        * UI에서 아이디어 입력 후 "분석 요청" 버튼 클릭 시, 실제 LLM API가 호출되고 (로그 또는 네트워크 트래픽 확인), 잠시 후 페르소나의 응답이 UI에 정상적으로 표시되는지 확인.
        * 오류 발생 시, 콘솔 또는 UI에 적절한 피드백이 나타나는지 확인 (Phase 4에서 상세 오류 처리).

---

## Phase 2: 멀티 페르소나 순차 실행 및 오케스트레이션 (워크플로우 구축)

* **목표**: 오케스트레이터 에이전트(ADK 워크플로우 에이전트)를 도입하여, 정의된 모든 MVP 페르소나가 사용자의 아이디어에 대해 순차적으로 의견을 제시하고, 각 페르소나가 이전 페르소나의 의견을 참고하여 분석을 진행하는 핵심 워크플로우를 완성합니다.
* **참고 워크플로우 항목**: Phase 2 전체

### 단계 2.1: 나머지 페르소나(`Marketer`, `Engineer`) `LlmAgent` 클래스 생성 및 시스템 프롬프트 설정
    * **지침**:
        1.  `src/agents` 디렉토리에 `marketer_agent.py` 및 `engineer_agent.py` 파일 생성.
        2.  각 파일에 창의적 마케터(아키텍처 `4.2.`) 및 현실적 엔지니어(아키텍처 `4.3.`) 페르소나를 위한 `LlmAgent` 클래스(예: `MarketerPersonaAgent`, `EngineerPersonaAgent`)를 단계 1.2, 1.3과 유사하게 구현.
        3.  각 페르소나의 시스템 프롬프트를 `config/prompts.py`에 추가하고, 각 에이전트 클래스에서 해당 프롬프트를 로드하여 설정. `instruction`은 이전 페르소나의 응답을 참조할 수 있도록 템플릿 변수(예: `{state.marketer_response}`, `{state.critic_response}`)를 포함해야 함.
        4.  각 에이전트의 `output_key`를 고유하게 설정 (예: `marketer_response`, `engineer_response`).
    * **커서룰**: II.1, II.5, III.2
    * **검증**:
        * 각 페르소나 에이전트가 개별적으로 (임의의 이전 페르소나 응답을 `session.state`에 수동 설정 후) 실행되었을 때, 의도한 시스템 프롬프트와 LLM 파라미터로 LLM을 호출하고 응답을 생성하는지 단위 테스트 또는 간단한 실행 스크립트로 확인.

### 단계 2.2: 오케스트레이터 에이전트 (`SequentialAgent` 또는 유사) 클래스 정의
    * **지침**:
        1.  `src/orchestrator` 디렉토리 생성. (커서룰 II.5)
        2.  `src/orchestrator/main_orchestrator.py` 파일 생성.
        3.  해당 파일에 Google ADK의 순차 실행 워크플로우 에이전트 (`SequentialAgent` 또는 ADK가 제공하는 유사 클래스)를 상속받거나 활용하는 오케스트레이터 클래스(예: `AIdeaLabOrchestrator`) 정의.
        4.  생성자에서 3개의 페르소나 에이전트(`CriticPersonaAgent`, `MarketerPersonaAgent`, `EngineerPersonaAgent`) 인스턴스를 생성하고, 실행 순서대로 리스트 또는 딕셔너리 형태로 관리. (아키텍처 `5.2. 구체적인 작동 방식 예시` 참고 - 마케터 -> 분석가 -> 엔지니어 순)
    * **참고 아키텍처**: 워크플로우 Phase 2 - 항목 3 (`Google ADK 워크플로우 상세 설계`)
    * **검증**:
        * 오케스트레이터 클래스가 오류 없이 인스턴스화되고, 내부에 페르소나 에이전트들이 올바른 순서로 포함되는지 디버거로 확인.

### 단계 2.3: 오케스트레이터의 페르소나 순차 호출 및 `session.state` 정보 전달 로직 구현
    * **지침**:
        1.  `AIdeaLabOrchestrator` 클래스 내에서 ADK 워크플로우 실행 로직(예: `run` 또는 `process` 메서드)을 구현.
        2.  해당 로직은 다음을 수행해야 함:
            * 초기 `session.state` (사용자 아이디어 포함)를 받음.
            * 정의된 순서대로 각 페르소나 에이전트를 실행.
            * **첫 번째 페르소나(마케터)** 실행 시: `session.state`의 `initial_idea`를 전달.
            * **두 번째 페르소나(분석가)** 실행 시: `session.state`의 `initial_idea`와 `marketer_response` (첫 페르소나의 결과)를 전달. 필요시 `marketer_response`가 너무 길 경우, 이를 요약하는 로직(LLM 호출 또는 간단한 문자열 처리)을 오케스트레이터에 추가하는 것을 고려 (아키텍처 `6.2.3. (선택적) 대화 히스토리 요약 시` 참고).
            * **세 번째 페르소나(엔지니어)** 실행 시: `session.state`의 `initial_idea`, `marketer_response`, `critic_response`를 전달 (필요시 요약).
            * 각 페르소나 실행 후, 해당 페르소나의 응답이 `session.state`에 올바르게 저장되는지 확인.
            * 모든 페르소나 실행 후, 최종 `session.state`에는 모든 페르소나의 응답이 포함되어야 함.
        3.  ADK 내부 호출 시 전달될 데이터 구조(메시지 스키마)는 아키텍처 `6.2.1. 하위 A2A 프로토콜 메시지 스키마`의 필드(session_id, task_type, idea_text, previous_dialogue_summary 등)를 참고하여 `session.state`를 통해 간접적으로 구현하거나, ADK가 명시적인 메시지 객체를 지원한다면 해당 방식 사용.
    * **커서룰**: II.3 (로직이 복잡해지면 내부 헬퍼 함수로 분리)
    * **검증**:
        * 단위 테스트: 가상의 `session.state`와 `initial_idea`를 오케스트레이터에 전달했을 때, 페르소나 에이전트들이 정의된 순서대로 호출되고(Mock 객체 사용), 각 페르소나에게 올바른 컨텍스트 정보(이전 응답 포함)가 전달되며, 최종 `session.state`에 모든 페르소나의 (Mock)응답이 기록되는지 검증.

### 단계 2.4: UI에서 오케스트레이터 실행 및 멀티 페르소나 결과 표시
    * **지침**:
        1.  `src/ui/app.py`의 "아이디어 분석 요청" 버튼 클릭 시 호출되는 함수를 수정.
        2.  단일 페르소나 에이전트 대신 `AIdeaLabOrchestrator` 인스턴스를 생성하고 실행.
        3.  오케스트레이터 실행 후, `session.state`에서 각 페르소나의 응답(`marketer_response`, `critic_response`, `engineer_response`)을 모두 가져옴.
        4.  UI에 각 페르소나의 이름과 함께 해당 응답을 순서대로 또는 의미 있는 그룹으로 묶어 표시. (워크플로우 Phase 2 - 항목 6 (`워크숍 진행 화면`) 참고)
        5.  각 페르소나 응답 생성 또는 전체 오케스트레이션 진행 중에 사용자에게 피드백을 주기 위한 로딩 인디케이터(예: `st.spinner("AI 페르소나들이 분석 중입니다...")`)를 UI에 추가.
    * **검증**:
        * UI에서 아이디어 입력 후 "분석 요청" 버튼 클릭 시, 오케스트레이터가 실행되고, 3명의 페르소나가 순차적으로 (또는 병렬 처리 후 순차 표시) 응답을 생성하며, 이 모든 응답이 UI에 명확하게 구분되어 표시되는지 확인.
        * 로딩 인디케이터가 적절한 시점에 표시되고 사라지는지 확인.

---

## Phase 3: 최종 요약 및 워크숍 기본 완료 (사용자 가치 제공)

* **목표**: 모든 페르소나의 순차 분석이 완료된 후, 오케스트레이터가 LLM을 활용하여 전체 논의 내용을 종합한 최종 요약 보고서를 생성하고, 이를 사용자에게 "워크숍 종료 및 최종 요약 보기" 옵션을 통해 제공합니다.
* **참고 워크플로우 항목**: Phase 3 전체

### 단계 3.1: 오케스트레이터에 최종 요약 생성 기능 추가 (LLM 호출)
    * **지침**:
        1.  `AIdeaLabOrchestrator` 클래스 또는 별도의 `SummaryAgent` (`LlmAgent` 상속) 클래스에 최종 요약 생성 로직 추가.
        2.  이 로직은 모든 페르소나의 응답이 `session.state`에 저장된 후 호출되어야 함.
        3.  입력: `session.state`의 `initial_idea`와 모든 페르소나의 응답 (`dialogue_history` 또는 `persona_responses`).
        4.  프롬프트: 아키텍처 `6.2.3. 최종 요약 생성 시`에 정의된 프롬프트 ("아이디어의 주요 장점, 단점, 핵심 피드백, 실행 가능한 다음 단계 제안 포함...")를 사용. 이 프롬프트도 `config/prompts.py`에 저장.
        5.  LLM 호출: Gemini 모델을 사용하여 요약 생성. LLM 파라미터는 아키텍처 `9.3. API 호출 시 파라미터 최종 값 결정` (오케스트레이터 요약 시: temperature 0.3~0.5, max_output_tokens 512~1024)을 참고하여 설정.
        6.  출력: 생성된 최종 요약 텍스트를 `session.state`의 새로운 키(예: `final_summary`)에 저장.
        7.  LLM 서비스 호출은 MCP 철학을 따라 일관된 방식으로 접근. 아키텍처 `9.2. Google ADK 내 LLM 서비스 도구(Tool) 인터페이스 정의`를 참고하여, 이 시점에서 `GeminiLLMTool` 클래스를 정의하고 사용하거나 ADK의 LLM 직접 호출 기능을 활용. 만약 `BaseTool`을 사용한다면 아키텍처 `10.4. 도구(Tool) 정의 및 등록 방식`을 따름.
    * **커서룰**: II.3 (요약 로직이 복잡하면 별도 모듈/클래스로 분리)
    * **검증**:
        * 단위 테스트: 가상의 페르소나 응답들을 `session.state`에 설정하고 요약 생성 기능을 호출했을 때, 올바른 요약 프롬프트가 생성되고 LLM(Mock 사용 가능) 호출 후, 생성된 (Mock)요약이 `session.state['final_summary']`에 저장되는지 검증.

### 단계 3.2: UI에 "최종 요약 보기" 버튼 및 요약 표시 화면 구현
    * **지침**:
        1.  `src/ui/app.py`의 워크숍 진행 화면 (아키텍처 `11.2.2.`) 수정:
            * 모든 페르소나의 의견이 표시된 후, 하단 또는 별도 사용자 액션 영역에 "최종 요약 보기" 버튼을 활성화.
        2.  "최종 요약 보기" 버튼 클릭 시 호출될 함수 구현:
            * 오케스트레이터의 최종 요약 생성 기능(단계 3.1에서 구현)을 호출하거나, 이미 `session.state`에 저장된 `final_summary`를 가져옴.
            * 가져온 최종 요약 텍스트를 새로운 UI 영역 또는 화면(아키텍처 `11.2.3. 최종 요약 보고서 화면`)에 표시. 요약 내용은 아키텍처에서 정의한 대로 장점, 단점, 핵심 피드백, 다음 단계 제안 등이 구조화되어 보이도록 마크다운 등을 활용하여 표시.
    * **검증**:
        * 멀티 페르소나 분석 완료 후 "최종 요약 보기" 버튼이 나타나는지 확인.
        * 버튼 클릭 시, (필요시 로딩 인디케이터 후) 최종 요약 보고서가 UI에 구조화된 형태로 정확히 표시되는지 확인.

---

## Phase 4: 기본 사용성 개선 및 안정화 (MVP 완성도 향상)

* **목표**: 기본적인 오류 처리 로직을 구현하고, 다양한 테스트를 통해 프롬프트 및 LLM 파라미터를 튜닝하여 응답 품질을 개선합니다. 또한 (선택적으로) 사용자 편의 기능을 추가하여 MVP의 전반적인 완성도와 안정성을 확보합니다.
* **참고 워크플로우 항목**: Phase 4 전체

### 단계 4.1: 기본 오류 처리 로직 구현
    * **지침**:
        1.  오케스트레이터 및 각 페르소나 에이전트의 LLM 호출 부분, ADK 내부 에이전트 호출 부분에 `try-except` 블록을 추가하여 예외 처리 구현 (아키텍처 `6.2.4. 오류 처리 로직 상세`).
        2.  LLM API 호출 실패 시 (네트워크 오류, API 키 문제, 서비스 제한 등):
            * 간단한 재시도 로직 구현 (예: 1~2회, 짧은 간격).
            * 재시도 실패 시, 사용자에게 "LLM 응답 생성에 실패했습니다. 잠시 후 다시 시도해주세요."와 같은 일반적인 오류 메시지를 UI에 표시.
            * 콘솔에는 상세 오류 로그 기록. (커서룰 VI.4)
        3.  ADK 내부 에이전트 호출 실패 시 (페르소나 간 호출):
            * 오류 발생 페르소나를 명시하고, 사용자에게 해당 페르소나의 분석이 누락되었음을 알림.
            * 가능하다면 나머지 분석이라도 계속 진행할지, 아니면 워크숍을 중단할지 사용자에게 옵션을 제공하거나, MVP에서는 단순 오류 알림 후 중단.
    * **검증**:
        * 의도적으로 LLM API 키를 틀리게 설정하거나 네트워크를 잠시 끊고 실행했을 때, UI에 적절한 오류 메시지가 표시되고 시스템이 비정상 종료되지 않는지 확인.
        * 특정 페르소나 에이전트에서 강제로 예외를 발생시켰을 때, 오케스트레이터가 이를 적절히 처리하는지 확인.

### 단계 4.2: 프롬프트 및 LLM 파라미터 튜닝
    * **지침**:
        1.  다양한 종류의 아이디어(짧은/긴, 구체적인/추상적인, 일반적인/전문적인)를 시스템에 입력하여 각 페르소나 및 최종 요약의 응답 품질을 주관적으로 평가.
        2.  응답 품질이 낮거나 페르소나의 역할/스타일이 제대로 반영되지 않는 경우, `config/prompts.py`의 해당 시스템 프롬프트를 수정하고 개선. (아키텍처 `9.5. 프롬프트 엔지니어링 및 관리`)
        3.  각 페르소나 및 오케스트레이터 요약 시 사용되는 LLM 파라미터(`temperature`, `max_output_tokens`, `top_k`, `top_p` 등)를 조정하며 응답 변화 관찰 및 최적값 탐색. (아키텍처 `9.3. API 호출 시 파라미터 최종 값 결정`)
        4.  효과적인 프롬프트와 파라미터 조합은 Git으로 버전 관리하고, 변경 이력을 기록.
    * **커서룰**: IV.1 (일관된 코딩 스타일 - 프롬프트 관리 방식도 일관되게)
    * **검증**:
        * 튜닝 전후의 응답을 비교하여 품질 개선 여부 확인.
        * 다양한 입력에 대해 일관되고 만족스러운 수준의 응답이 생성되는지 반복 테스트.

### 단계 4.3: 비용 관리 및 모니터링 인지
    * **지침**:
        1.  Google Cloud Console (또는 Gemini API 대시보드)에 접속하여 API 사용량, 호출 횟수, 발생 비용 등을 확인하는 방법을 숙지. (아키텍처 `9.4. 비용 관리 및 모니터링 전략`)
        2.  개발 및 테스트 중 불필요한 API 호출을 최소화하도록 주의.
        3.  (선택적) API 호출 시 입/출력 토큰 수를 로깅하는 간단한 로직 추가하여 비용 분석에 활용.
    * **검증**:
        * 실제 API 대시보드에서 테스트 중 발생한 호출 건수 및 (예상)비용 확인 가능 여부.

### 단계 4.4: UI/UX 개선 (오류 메시지, 피드백 등)
    * **지침**:
        1.  단계 4.1에서 구현한 오류 발생 시, UI에 표시되는 메시지가 사용자 친화적인지 검토하고 개선. (아키텍처 `11.2.2. UX 고려사항 중 오류 상황 UI/UX`)
        2.  시스템이 백그라운드에서 작업 중일 때(예: 여러 페르소나 순차 분석, 최종 요약 생성) 사용자에게 명확한 진행 상태 피드백(이미 구현된 로딩 인디케이터 외 추가적인 텍스트 안내 등)을 제공하는 방안 고려 및 개선.
        3.  전반적인 UI의 가독성, 사용 편의성 등을 아키텍처 `11.5. 디자인 고려사항`에 맞춰 점검하고 간단한 개선 적용.
    * **검증**:
        * 오류 발생 시 또는 긴 작업 시 UI 피드백이 명확하고 사용자를 안심시키는지 확인.
        * 개선된 UI가 이전보다 사용하기 편하고 보기 좋은지 주관적 평가.

### 단계 4.5: (선택적 MVP 기능) "아이디어 수정 후 재시작" 기능 구현
    * **지침**:
        1.  아키텍처 `6.2.2. "다음 단계 제안" 로직 상세화`의 `2. "아이디어 수정 후 처음부터 다시 검토 시작"` 옵션을 구현.
        2.  UI의 최종 요약 화면 또는 워크숍 진행 중 적절한 시점에 "아이디어 수정 후 재시작" 버튼 추가.
        3.  버튼 클릭 시:
            * 사용자가 아이디어를 수정 입력할 수 있는 인터페이스 제공 (기존 입력창 재활용 또는 새 입력창).
            * `AIdeaLabOrchestrator` 또는 UI 로직에서 `session.state`의 이전 대화 기록(`dialogue_history`, `persona_responses` 등)을 초기화하거나, 새로운 세션으로 처리.
            * 수정된 아이디어를 `session.state['initial_idea']`로 설정.
            * Phase 2의 오케스트레이션 프로세스(멀티 페르소나 분석)를 다시 시작.
    * **검증**:
        * "아이디어 수정 후 재시작" 버튼 클릭 후, 새 아이디어로 페르소나 분석이 정상적으로 다시 시작되고 이전 분석 내용과 섞이지 않는지 확인.

### 단계 4.6: (선택적 MVP 기능) 간단한 세션 데이터 저장 (SQLite)
    * **지침**:
        1.  아키텍처 `10.4. 도구(Tool) 정의 및 등록 방식`을 참고하여 `DatabaseAccessTool` 클래스를 `src/tools` 디렉토리에 `google.adk.BaseTool`을 상속하여 구현.
        2.  이 도구는 SQLite DB에 연결하고, 워크숍 세션 데이터(초기 아이디어, 각 페르소나 응답, 최종 요약 등)를 저장하는 메서드(예: `save_session_data(session_data: dict)`)를 포함.
        3.  오케스트레이터 (`AIdeaLabOrchestrator`)가 최종 요약 생성 후 또는 워크숍 종료 시점에 `DatabaseAccessTool`을 사용하여 `session.state`의 주요 내용을 DB에 저장하도록 로직 추가.
        4.  SQLite DB 파일은 프로젝트 루트 또는 `data` 디렉토리에 생성되도록 설정.
    * **참고 아키텍처**: `3. 시스템 아키텍처`의 `DB_Tool` 컴포넌트.
    * **커서룰**: II.1, II.5 (도구 클래스 분리)
    * **검증**:
        * 워크숍 완료 후 SQLite DB 파일이 생성되고, 내부 테이블에 세션 데이터가 올바르게 저장되었는지 DB 브라우저 등으로 확인.
        * (로드 기능은 MVP 이후)

---

## Phase 5: 확장 및 고도화 (MVP 이후)

* **목표**: MVP의 핵심 기능을 기반으로 사용자 피드백을 반영하여 기능을 확장하고, 시스템의 성능과 안정성을 최적화하며, 장기적인 발전 방향을 모색하여 "AIdea Lab"의 가치를 더욱 높입니다.
* **지침**: 아래 항목들은 MVP 완료 후, 우선순위 및 필요에 따라 선택적으로 진행합니다. 각 항목 구현 시에는 아키텍처 문서의 해당 상세 내용을 다시 한번 정독하고, 커서룰에 따라 체계적으로 개발합니다.
    1.  **A2A Protocol 연동 연구 및 적용** (아키텍처 `2. 핵심 철학 - A2A`, `5.2. 확장 옵션`)
    2.  **외부 데이터 API 연동 도구 개발** (아키텍처 `3. 시스템 아키텍처 - External Data API Tools`)
    3.  **ADK 라우터 에이전트 활용 심층 논의 기능** (아키텍처 `5.2. 확장 옵션`)
    4.  **긴 대화 히스토리 요약 기능 고도화** (아키텍처 `10.3. 상태 관리 - 긴 대화 처리`)
    5.  **Google ADK 기반 테스트 및 평가 도구 도입** (아키텍처 `10.5.`)
    6.  **UI 고급 기능 추가** (이전 세션 불러오기, 결과 저장/다운로드 등 - 아키텍처 `11.2.1`, `11.2.3`)
    7.  **UI 기술 스택 확장 고려** (React/Next.js + FastAPI/Flask 등 - 아키텍처 `11. UI 설계 확장`)
* **검증**: 각 확장 기능별로 구체적인 테스트 케이스를 정의하고, 기능이 의도대로 작동하며 기존 시스템에 부정적인 영향을 미치지 않는지 철저히 검증.

---

